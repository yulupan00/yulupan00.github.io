<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type"
      content="text/html; charset=UTF-8">

    <title>Yulu Pan Personal Website</title>

    <meta name="author" content="Yulu Pan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table
      style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:63%;vertical-align:middle">
                    <p style="text-align:center">
                      <name>Yulu Pan</name>
                    </p>
                    <p>I'm a PhD student at the University of
                      North Carolina at Chapel Hill in the Department of
                      Computer Science.<br>
                      <p>I'm currently working with <a
                          href="https://www.gedasbertasius.com/"> Prof. Gedas
                          Bertasius</a> on  AI
                          for sports, fine-grained skill analysis and video understanding. I'm also a research assistant at <a
                          href="https://zylkalab.org/">Zylka lab</a> on
                        developing computer vision models for spontaneous pain
                        measurement from mouse.</p>
                    </p>
                    <p>Previously, I graduated from UNC in May 2023 with a B.S. in Computer
                      Science and a B.A. in Mathematics, and received
                      M.S. in Computer Science in May 2025. I worked with <a
                        href="https://www.cs.unc.edu/~ronisen/">Prof. Roni
                        Sengupta</a> on computer vision during my undergraduate
                      studies.
                    </p>

                    <p style="text-align:center">
                      <a href="mailto:yulupan@cs.unc.edu">Email</a> &nbsp/&nbsp
                      <a href="data/resume.pdf">CV</a> &nbsp/&nbsp
                      <a
                        href="https://scholar.google.com/citations?user=ENsyaDcAAAAJ&hl=en&oi=ao">Google
                        Scholar</a> &nbsp/&nbsp
                      <a href="https://github.com/yulupan00">Github</a>
                      &nbsp/&nbsp
                      <a
                        href="https://www.linkedin.com/in/yulu-pan-00">LinkedIn</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:40%;max-width:40%">
                    <a><img
                        style="width:100%;max-width:100%;border-radius: 20px;"
                        alt="profile photo" src="images/yulu.png"></a>
                  </td>
                </tr>
              </tbody></table>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Research</heading>
                    <p>
                      I'm interested in <b>Computer Vision</b>,
                      <b>Video Understanding</b>, and <b>AI for Sports</b>. My current
                      focus is on fine-grained video understanding, particularly in
                      the areas of sports analytics. I'm interested in how to leverage AI to 
                      improve the understadning of complex, fine-grained human action in sports.
                    </p>
                  </td>
                </tr>
              </tbody></table>
            <table
              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publication</heading>

                  <table
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                      
                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id="basket_image">
                            <img src="images/exact.png" alt="exact img">
                          </div>                          
                        </div>

                      </td>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://yulupan00.github.io/">
                        <papertitle>ExAct: A Video-Language Benchmark for Expert Action Analysis</papertitle>
                      </a>
                      <br>
                      Han Yi, <strong>Yulu Pan</strong>, Feihong He, Xinyu Liu, Benjamin Zhang, Oluwatumininu Oguntola, Gedas Bertasius
                      <br>
                      <em>arXiv, 2025</em>
                      <br>
                      <a href="https://texaser.github.io/exact_project_page/"
                        target="+black">Project Page</a>
                      /
                      <a href="https://arxiv.org/abs/2506.06277"
                        target="_blank">Paper</a>
                      /
                      <a href="https://huggingface.co/datasets/Alexhimself/ExAct"
                        target="+black">Data</a>
                      <p></p>
                      <p>
                        We introduce ExAct, a video-language benchmark for expert-level analysis of skilled human actions. 
                        It contains over 3,500 expert-curated video QA pairs across domains like sports, cooking, and music. 
                        Our benchmark reveals a significant performance gap between state-of-the-art VLMs and human experts, 
                        highlighting the need for models with a more nuanced understanding of complex human skills.
                      </p>
                    </td>
                  </tr>

                      <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                          <div class="two" id="basket_image">
                            <img src="images/BASKET.gif" alt="Basket GIF">
                          </div>                          
                        </div>

                      </td>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://yulupan00.github.io/">
                        <papertitle>BASKET: A Large-Scale Video Dataset for
                          Fine-Grained Skill Estimation</papertitle>
                      </a>
                      <br>
                      <strong>Yulu Pan</strong>, Ce Zhang, Gedas Bertasius
                      <br>
                      <em>CVPR 2025</em>
                      <br>
                      <a href="https://sites.google.com/cs.unc.edu/basket"
                        target="+black">Project Page</a>
                      /
                      <a href="https://arxiv.org/abs/2503.20781"
                        target="_blank">Paper</a>
                      /
                      <a href="https://github.com/yulupan00/BASKET"
                        target="+black">Code & Data</a>
                      <p></p>
                      <p>
                        We present BASKET, a large-scale basketball video
                        dataset
                        for fine-grained skill estimation. BASKET contains more
                        than 4,400 hours of video capturing 32,232 basketball
                        players from all over the world. We benchmark multiple
                        SOTA video recognition models and reveal that these
                        models
                        struggle to achieve good results on our benchmark.
                      </p>
                    </td>
                  </tr>

                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='motion_matters_image'><video
                          width=120% height=120% muted autoplay loop>
                          <source src="images/motion_matters.mp4"
                            type="video/mp4">
                          Your browser does not support the video tag.
                        </video></div>
                    </div>

                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="http://motion-matters.github.io/">
                      <papertitle>Motion Matters: Neural Motion Transfer for
                        Better Camera Physiological Sensing</papertitle>
                    </a>
                    <br>
                    Akshay Paruchuri, Xin Liu, <strong>Yulu Pan</strong>,
                    Shwetak Patel, Daniel McDuff*, Soumyadip Sengupta*
                    <br>
                    <em>WACV</em>, 2024, Oral, Top 2.6%
                    <br>
                    <a href="https://motion-matters.github.io/"
                      target="+black">Project Page</a>
                    /
                    <a href="https://arxiv.org/abs/2303.12059"
                      target="_blank">Paper</a>
                    /
                    <a href="https://github.com/Roni-Lab/MA-rPPG-Video-Toolbox"
                      target="+black">Code</a>
                    <p></p>
                    <p>
                      Neural Motion Transfer serves as an effective data
                      augmentation technique for PPG signal estimation from
                      facial videos. We devise the best strategy to augment
                      publicly available datasets with motion augmentation,
                      improving up to 75% over SOTA techniques on five benchmark
                      datasets.
                    </p>
                  </td>
                </tr>

                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id="basket_image">
                      <img
                        src="images/Semi-Supervised_Semantic_Segmentation.png"
                        alt="semi Image">
                    </div>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yulupan00.github.io/">
                    <papertitle>Semi-Supervised Semantic Segmentation with
                      Multi-Reliability and Multi-Level Feature
                      Augmentation</papertitle>
                  </a>
                  <br>
                  JianJian Yin, Zhichao Zheng, <strong>Yulu Pan</strong>,
                  Yanhui Gu, Yi Chen*
                  <br>
                  <em>Expert Systems with Applications</em>, Volume 233, 15
                  December 2023, 120973

                  <br>
                  <a
                    href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4247750"
                    target="_blank">Paper</a>
                  /
                  <a href="https://github.com/JianJianYin/MMFA"
                    target="+black">Code</a>
                  <p></p>
                  <p>
                    Introducing a multi-reliability and multi-level feature
                    augmentation framework for semi-supervised semantic
                    segmentation, effectively utilizing labeled and unlabeled
                    images and improving segmentation performance on benchmark
                    datasets.
                  </p>
                </td>
              </tr>

            </tbody></table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Misc</heading>
                  <p>
                    I am enthusiastic in helping other students succeed in
                    computer science. I have shared my knowledge and support
                    students' learning journey in the following course:
                  </p>
                  <p>
                    <p>University of North Carolina at Chapel Hill,
                      Undergraduate Learning Assistant:</p>
                    <ul>
                      <li><a href="https://www.comp301.com/">COMP 301</a>:
                        Foundations of Programming</li>
                      <li>COMP 116: Introduction to Scientific
                        Programming</li>
                    </ul>

                    <p>Brandeis University, Teaching Assistant:</p>
                    <ul>
                      <li>COSI 21A: Data Structures and the Fundamentals of
                        Computing</li>

                    </ul>

                  </tbody></table>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                    <tr>
                      <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                          Cloned from <a
                            href="https://github.com/jonbarron/jonbarron_website">Jon
                            Barron</a>.
                        </p>
                      </td>
                    </tr>
                  </tbody></table>
              </td>
            </tr>
          </table>
        </body>

      </html>
